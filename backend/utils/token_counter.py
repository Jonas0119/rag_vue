"""
Token ç»Ÿè®¡æ¨¡å—
ç”¨äºç»Ÿè®¡æ¨¡å‹è°ƒç”¨çš„è¾“å…¥å’Œè¾“å‡º token æ•°é‡
"""

from typing import Dict, Any
from collections import defaultdict
from langchain_core.messages import BaseMessage


class TokenCounter:
    """Token ç»Ÿè®¡å™¨ï¼Œç”¨äºè·Ÿè¸ªæ¨¡å‹è°ƒç”¨çš„ token ä½¿ç”¨æƒ…å†µ"""
    
    def __init__(self):
        """åˆå§‹åŒ– Token ç»Ÿè®¡å™¨"""
        # æŒ‰æ¨¡å‹ç±»å‹ç»Ÿè®¡ï¼šmodel, response_model, grader_model
        self.model_stats: Dict[str, Dict[str, int]] = defaultdict(lambda: {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "call_count": 0
        })
        
        # æŒ‰èŠ‚ç‚¹ç»Ÿè®¡ï¼šgenerate_query_or_respond, grade_documents, rewrite_question, generate_answer
        self.node_stats: Dict[str, Dict[str, int]] = defaultdict(lambda: {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "call_count": 0
        })
        
        # æ€»ä½“ç»Ÿè®¡
        self.total_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "call_count": 0
        }
    
    def count_tokens(
        self,
        model_name: str,
        node_name: str,
        input_tokens: int,
        output_tokens: int,
    ):
        """
        ç»Ÿè®¡ token ä½¿ç”¨æƒ…å†µ
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆmodel, response_model, grader_modelï¼‰
            node_name: èŠ‚ç‚¹åç§°ï¼ˆgenerate_query_or_respond, grade_documents, rewrite_question, generate_answerï¼‰
            input_tokens: è¾“å…¥ token æ•°é‡
            output_tokens: è¾“å‡º token æ•°é‡
        """
        total_tokens = input_tokens + output_tokens
        
        # æ›´æ–°æ¨¡å‹ç»Ÿè®¡
        self.model_stats[model_name]["input_tokens"] += input_tokens
        self.model_stats[model_name]["output_tokens"] += output_tokens
        self.model_stats[model_name]["total_tokens"] += total_tokens
        self.model_stats[model_name]["call_count"] += 1
        
        # æ›´æ–°èŠ‚ç‚¹ç»Ÿè®¡
        self.node_stats[node_name]["input_tokens"] += input_tokens
        self.node_stats[node_name]["output_tokens"] += output_tokens
        self.node_stats[node_name]["total_tokens"] += total_tokens
        self.node_stats[node_name]["call_count"] += 1
        
        # æ›´æ–°æ€»ä½“ç»Ÿè®¡
        self.total_stats["input_tokens"] += input_tokens
        self.total_stats["output_tokens"] += output_tokens
        self.total_stats["total_tokens"] += total_tokens
        self.total_stats["call_count"] += 1
    
    def get_model_response_tokens(self, response: Any) -> tuple[int, int]:
        """
        ä»æ¨¡å‹å“åº”ä¸­æå– token æ•°é‡
        
        ä¼˜å…ˆçº§ï¼š
        1. usage_metadataï¼ˆLangChain 1.0+ï¼‰
        2. response_metadata['usage'] æˆ– response_metadata['token_usage']
        3. ä¼°ç®—
        """
        input_tokens = 0
        output_tokens = 0
        
        # 1) å°è¯• usage_metadataï¼ˆæ”¯æŒå¯¹è±¡å’Œå­—å…¸ä¸¤ç§æ ¼å¼ï¼‰
        if hasattr(response, "usage_metadata") and response.usage_metadata:
            usage = response.usage_metadata
            # æ”¯æŒå­—å…¸æ ¼å¼
            if isinstance(usage, dict):
                input_tokens = usage.get("input_tokens", 0) or 0
                output_tokens = usage.get("output_tokens", 0) or 0
            else:
                # æ”¯æŒå¯¹è±¡æ ¼å¼ï¼ˆä½¿ç”¨ getattr å®‰å…¨è®¿é—®ï¼‰
                input_tokens = getattr(usage, "input_tokens", 0) or 0
                output_tokens = getattr(usage, "output_tokens", 0) or 0
        
        # 2) å°è¯• response_metadata
        if (input_tokens == 0 and output_tokens == 0) and hasattr(response, 'response_metadata'):
            metadata = response.response_metadata
            if metadata:
                # å°è¯•å¤šç§å¯èƒ½çš„ token ç»Ÿè®¡å­—æ®µ
                input_tokens = (
                    metadata.get('input_tokens') or
                    metadata.get('usage', {}).get('input_tokens') or
                    metadata.get('token_usage', {}).get('input_tokens') or
                    0
                )
                output_tokens = (
                    metadata.get('output_tokens') or
                    metadata.get('usage', {}).get('output_tokens') or
                    metadata.get('token_usage', {}).get('output_tokens') or
                    0
                )
        
        # 3) å¦‚æœæ²¡æœ‰æ‰¾åˆ° token ä¿¡æ¯ï¼Œå°è¯•ä»æ¶ˆæ¯ä¸­ä¼°ç®—
        if input_tokens == 0 and output_tokens == 0:
            if hasattr(response, 'content'):
                content = str(response.content)
                # æ›´å‡†ç¡®çš„ä¸­æ–‡ token ä¼°ç®—
                estimated_tokens = self._estimate_tokens_chinese(content)
                output_tokens = int(estimated_tokens)
        
        return int(input_tokens), int(output_tokens)
    
    def _estimate_tokens_chinese(self, text: str) -> float:
        """
        æ›´å‡†ç¡®åœ°ä¼°ç®—ä¸­æ–‡æ–‡æœ¬çš„ token æ•°é‡
        
        å¯¹äºä¸­æ–‡æ–‡æœ¬ï¼š
        - ä¸­æ–‡å­—ç¬¦ï¼šé€šå¸¸ 1 ä¸ªä¸­æ–‡å­—ç¬¦ â‰ˆ 1.5-2 tokensï¼ˆå–å†³äº tokenizerï¼‰
        - è‹±æ–‡/æ•°å­—ï¼šé€šå¸¸ 1 ä¸ªå­—ç¬¦ â‰ˆ 0.25-0.5 tokens
        - æ ‡ç‚¹ç¬¦å·ï¼šé€šå¸¸ 1 ä¸ªæ ‡ç‚¹ â‰ˆ 0.5-1 token
        
        è¿™é‡Œä½¿ç”¨æ··åˆä¼°ç®—æ–¹æ³•ï¼š
        - ä¸­æ–‡å­—ç¬¦ï¼ˆCJKç»Ÿä¸€æ±‰å­—ï¼‰ï¼šæŒ‰ 1.8 tokens/å­—ç¬¦
        - å…¶ä»–å­—ç¬¦ï¼šæŒ‰ 0.4 tokens/å­—ç¬¦
        
        Args:
            text: å¾…ä¼°ç®—çš„æ–‡æœ¬
            
        Returns:
            ä¼°ç®—çš„ token æ•°é‡
        """
        if not text:
            return 0.0
        
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡ï¼ˆCJKç»Ÿä¸€æ±‰å­—èŒƒå›´ï¼‰
        chinese_chars = 0
        other_chars = 0
        
        for char in text:
            # CJKç»Ÿä¸€æ±‰å­—èŒƒå›´ï¼š\u4e00-\u9fff
            # è¿˜åŒ…æ‹¬æ‰©å±•Aï¼š\u3400-\u4dbf
            if '\u4e00' <= char <= '\u9fff' or '\u3400' <= char <= '\u4dbf':
                chinese_chars += 1
            elif char.strip():  # éç©ºç™½å­—ç¬¦
                other_chars += 1
        
        # ä¸­æ–‡å­—ç¬¦æŒ‰ 1.8 tokens/å­—ç¬¦ï¼Œå…¶ä»–å­—ç¬¦æŒ‰ 0.4 tokens/å­—ç¬¦
        estimated_tokens = chinese_chars * 1.8 + other_chars * 0.4
        return estimated_tokens
    
    def get_messages_tokens(self, messages: list[BaseMessage]) -> int:
        """
        ä¼°ç®—æ¶ˆæ¯åˆ—è¡¨çš„ token æ•°é‡
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            ä¼°ç®—çš„ token æ•°é‡
        """
        total_chars = 0
        for msg in messages:
            if hasattr(msg, 'content'):
                total_chars += len(str(msg.content))
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                # å·¥å…·è°ƒç”¨ä¹Ÿä¼šæ¶ˆè€— token
                total_chars += len(str(msg.tool_calls)) * 2
        
        # ä½¿ç”¨æ›´å‡†ç¡®çš„ä¸­æ–‡ token ä¼°ç®—
        content_str = ""
        for msg in messages:
            if hasattr(msg, 'content'):
                content_str += str(msg.content)
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                content_str += str(msg.tool_calls)
        
        return self._estimate_tokens_chinese(content_str)
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ç»“æœ"""
        print("\n" + "=" * 80)
        print("ğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡")
        print("=" * 80)
        
        # æ€»ä½“ç»Ÿè®¡
        print("\nã€æ€»ä½“ç»Ÿè®¡ã€‘")
        print(f"  æ€»è°ƒç”¨æ¬¡æ•°: {self.total_stats['call_count']}")
        print(f"  æ€»è¾“å…¥ Token: {self.total_stats['input_tokens']:,}")
        print(f"  æ€»è¾“å‡º Token: {self.total_stats['output_tokens']:,}")
        print(f"  æ€» Token: {self.total_stats['total_tokens']:,}")
        
        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        if self.model_stats:
            print("\nã€æŒ‰æ¨¡å‹ç»Ÿè®¡ã€‘")
            for model_name, stats in self.model_stats.items():
                print(f"  {model_name}:")
                print(f"    è°ƒç”¨æ¬¡æ•°: {stats['call_count']}")
                print(f"    è¾“å…¥ Token: {stats['input_tokens']:,}")
                print(f"    è¾“å‡º Token: {stats['output_tokens']:,}")
                print(f"    æ€» Token: {stats['total_tokens']:,}")
        
        # æŒ‰èŠ‚ç‚¹ç»Ÿè®¡
        if self.node_stats:
            print("\nã€æŒ‰èŠ‚ç‚¹ç»Ÿè®¡ã€‘")
            for node_name, stats in self.node_stats.items():
                print(f"  {node_name}:")
                print(f"    è°ƒç”¨æ¬¡æ•°: {stats['call_count']}")
                print(f"    è¾“å…¥ Token: {stats['input_tokens']:,}")
                print(f"    è¾“å‡º Token: {stats['output_tokens']:,}")
                print(f"    æ€» Token: {stats['total_tokens']:,}")
        
        print("=" * 80 + "\n")
    
    def reset(self):
        """é‡ç½®æ‰€æœ‰ç»Ÿè®¡"""
        self.model_stats.clear()
        self.node_stats.clear()
        self.total_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "call_count": 0
        }


# å…¨å±€ Token ç»Ÿè®¡å™¨å®ä¾‹
token_counter = TokenCounter()



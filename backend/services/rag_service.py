"""
RAG æœåŠ¡ - æ£€ç´¢å¢å¼ºç”Ÿæˆ
"""
import os
import uuid
import logging
from typing import List, Dict, Optional, Generator
import time

from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.messages import HumanMessage, ToolMessage

from backend.utils.config import config
from backend.utils.prompts import RAG_TEMPLATE, DIRECT_ANSWER_TEMPLATE
from .vector_store_service import get_vector_store_service

logger = logging.getLogger(__name__)

# LangGraph RAGï¼ˆå¯é€‰ï¼‰
from .rag_graph import build_rag_graph
from .rag_nodes import (
    create_generate_query_or_respond_node,
    create_grade_documents_node,
    create_rewrite_question_node,
    create_generate_answer_node,
)
from .rag_tools import create_retrieve_tool
from .hybrid_retriever import HybridRetriever
from .reranker import CrossEncoderReranker, RemoteReranker
from .checkpoint_manager import create_checkpointer
from backend.database import ParentChildDAO
from backend.utils.token_counter import token_counter


class RAGService:
    """RAG é—®ç­”æœåŠ¡"""
    
    def __init__(self):
        self.vector_service = get_vector_store_service()
        self.llm = self._init_llm()
        self.summary_llm = self._init_summary_llm()  # ç”¨äºæ¶ˆæ¯æ€»ç»“çš„æ¨¡å‹
        self.prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)
        self.direct_prompt = ChatPromptTemplate.from_template(DIRECT_ANSWER_TEMPLATE)
        self.parent_child_dao = ParentChildDAO()
    
    def _init_llm(self):
        """åˆå§‹åŒ– LLM"""
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["ANTHROPIC_API_KEY"] = config.ANTHROPIC_API_KEY
        os.environ["ANTHROPIC_BASE_URL"] = config.ANTHROPIC_BASE_URL
        
        llm = ChatAnthropic(
            model=config.LLM_MODEL,
            temperature=config.LLM_TEMPERATURE,
            max_tokens=config.LLM_MAX_TOKENS
        )
        return llm
    
    def _init_summary_llm(self):
        """åˆå§‹åŒ–ç”¨äºæ€»ç»“çš„ LLMï¼ˆå¦‚æœå¯ç”¨æ¶ˆæ¯æ€»ç»“ï¼‰"""
        if not config.USE_MESSAGE_SUMMARIZATION:
            return None
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["ANTHROPIC_API_KEY"] = config.ANTHROPIC_API_KEY
        os.environ["ANTHROPIC_BASE_URL"] = config.ANTHROPIC_BASE_URL
        
        # åˆ›å»ºæ€»ç»“ç”¨çš„æ¨¡å‹
        if config.MESSAGE_SUMMARIZATION_MODEL:
            summary_model = ChatAnthropic(
                model=config.MESSAGE_SUMMARIZATION_MODEL,
                temperature=0.0,  # æ€»ç»“ä½¿ç”¨è¾ƒä½æ¸©åº¦
                max_tokens=config.MESSAGE_SUMMARIZATION_MAX_TOKENS
            )
        else:
            # å¦‚æœæ²¡æœ‰é…ç½®æ€»ç»“æ¨¡å‹ï¼Œä½¿ç”¨ä¸»æ¨¡å‹
            summary_model = self.llm
        
        return summary_model

    def _build_langgraph_graph(self, user_id: int):
        """
        ä¸ºæŒ‡å®šç”¨æˆ·æ„å»º LangGraph RAG å·¥ä½œæµï¼ˆæŒ‰é…ç½®é€‰æ‹© retriever / reranker / parent-childï¼‰ã€‚
        """
        # retriever
        if config.USE_HYBRID_RETRIEVER:
            retriever = HybridRetriever(user_id=user_id, top_k=config.HYBRID_RETRIEVER_TOP_K)
        else:
            retriever = self.vector_service.get_retriever(user_id, k=config.HYBRID_RETRIEVER_TOP_K)

        # parent-child æ˜ å°„ï¼šç»™ retriever æ³¨å…¥ parent_mapï¼ˆå·¥å…·ä¼šç”¨æ¥ä» child æ˜ å°„å› parentï¼‰
        if config.USE_PARENT_CHILD_STRATEGY and hasattr(retriever, "set_parent_map"):
            parent_map = self.parent_child_dao.get_parent_map_for_user(user_id)
            try:
                retriever.set_parent_map(parent_map)  # type: ignore[attr-defined]
            except Exception:
                pass

        # rerankerï¼ˆå¯é€‰ï¼‰
        reranker = None
        if config.USE_RERANKER:
            if config.USE_REMOTE_RERANKER and config.INFERENCE_API_BASE_URL:
                reranker = RemoteReranker(
                    base_url=config.INFERENCE_API_BASE_URL,
                    api_key=config.INFERENCE_API_KEY,
                    timeout=config.INFERENCE_API_TIMEOUT,
                    max_retry=config.INFERENCE_API_MAX_RETRY,
                )
            else:
                reranker = CrossEncoderReranker()

        retrieve_tool = create_retrieve_tool(
            retriever=retriever,
            reranker=reranker,
            top_k=config.RERANK_TOP_K,
            top_n=config.RERANK_TOP_N,
            rerank_score_threshold=config.RERANK_SCORE_THRESHOLD,
        )

        # èŠ‚ç‚¹æ¨¡å‹ï¼šç›®å‰ç›´æ¥å¤ç”¨åŒä¸€ä¸ª ChatAnthropicï¼ˆMiniMax-M2 via Anthropicï¼‰
        response_model = self.llm
        grader_model = self.llm

        generate_query_or_respond = create_generate_query_or_respond_node(response_model, retrieve_tool)
        grade_documents = create_grade_documents_node(grader_model, debug=False)
        rewrite_question = create_rewrite_question_node(response_model)
        generate_answer = create_generate_answer_node(response_model)
        
        # åˆ›å»ºæ¶ˆæ¯æ€»ç»“èŠ‚ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        summarize_messages = None
        if config.USE_MESSAGE_SUMMARIZATION and self.summary_llm:
            from .rag_nodes import create_summarize_messages_node
            summarize_messages = create_summarize_messages_node(self.summary_llm)
            logger.info(f"[RAGService] å·²åˆ›å»ºæ¶ˆæ¯æ€»ç»“èŠ‚ç‚¹ï¼Œè§¦å‘é˜ˆå€¼: {config.MESSAGE_SUMMARIZATION_THRESHOLD} tokensï¼Œä¿ç•™æ¶ˆæ¯æ•°: {config.MESSAGE_SUMMARIZATION_KEEP_MESSAGES}")

        # åˆ›å»º checkpointerï¼ˆå¦‚æœå¯ç”¨ï¼‰
        checkpointer = create_checkpointer()

        graph = build_rag_graph(
            retrieve_tool=retrieve_tool,
            generate_query_or_respond_node=generate_query_or_respond,
            grade_documents_node=grade_documents,
            rewrite_question_node=rewrite_question,
            generate_answer_node=generate_answer,
            summarize_messages_node=summarize_messages,
            checkpointer=checkpointer,
        )
        return graph

    @staticmethod
    def _parse_retrieve_output(text: str) -> List[Dict]:
        """
        å°†å·¥å…·è¾“å‡ºçš„æ ¼å¼åŒ–æ–‡æœ¬è§£æä¸º UI éœ€è¦çš„ retrieved_docsã€‚
        ä»å·¥å…·è¾“å‡ºä¸­æå– rerank_score å¹¶è½¬æ¢ä¸º similarityã€‚
        """
        if not text or "No relevant documents found." in text:
            return []
        parts = text.split("\n\n[Document ")
        docs = []
        for idx, part in enumerate(parts):
            if idx == 0:
                chunk = part
            else:
                chunk = "[Document " + part
            
            # è§£æ header å’Œ content
            if "\n" in chunk:
                header_line, content = chunk.split("\n", 1)
            else:
                header_line = chunk
                content = ""
            
            content = content.strip()
            if not content:
                continue
            
            # ä» header ä¸­æå–å…ƒæ•°æ®
            # æ ¼å¼: [Document {i}] (Source: ..., Rerank_score: {value}, ...)
            import re
            metadata = {}
            similarity = None
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– rerank_score
            # åŒ¹é… "Rerank_score: {æ•°å­—}" æˆ– "Rerank_score:{æ•°å­—}"
            rerank_match = re.search(r'Rerank_score:\s*([+-]?\d*\.?\d+)', header_line)
            if rerank_match:
                try:
                    rerank_score = float(rerank_match.group(1))
                    metadata["rerank_score"] = rerank_score
                    
                    # å°† rerank_score è½¬æ¢ä¸º similarity (0-1 èŒƒå›´)
                    # Cross-Encoder çš„åˆ†æ•°é€šå¸¸æ˜¯ç›¸å…³æ€§åˆ†æ•°ï¼Œè¶Šé«˜è¶Šç›¸å…³
                    # å¯¹äº BAAI/bge-reranker-base ç­‰æ¨¡å‹ï¼Œåˆ†æ•°é€šå¸¸åœ¨ -10 åˆ° 10 ä¹‹é—´
                    # ä½¿ç”¨ sigmoid å‡½æ•°å½’ä¸€åŒ–åˆ° 0-1 èŒƒå›´
                    import math
                    # sigmoid: 1 / (1 + exp(-x))
                    # ä¸ºäº†æ›´å¥½çš„æ˜¾ç¤ºæ•ˆæœï¼Œæˆ‘ä»¬å¯ä»¥è°ƒæ•´ sigmoid çš„ç¼©æ”¾
                    # ä½¿ç”¨ tanh çš„å˜ä½“ï¼šå°†åˆ†æ•°æ˜ å°„åˆ° 0-1
                    similarity = 1 / (1 + math.exp(-rerank_score))
                    
                    # ç¡®ä¿ similarity åœ¨ 0-1 èŒƒå›´å†…
                    similarity = max(0.0, min(1.0, similarity))
                except (ValueError, TypeError) as e:
                    logger.warning(f"[è§£æå·¥å…·è¾“å‡º] æ— æ³•è§£æ Rerank_score: {e}, header: {header_line[:100]}")
            
            # æå–å…¶ä»–å…ƒæ•°æ®ï¼ˆSource, Title ç­‰ï¼‰
            source_match = re.search(r'Source:\s*([^,)]+)', header_line)
            if source_match:
                metadata["source"] = source_match.group(1).strip()
            
            title_match = re.search(r'Title:\s*([^,)]+)', header_line)
            if title_match:
                metadata["title"] = title_match.group(1).strip()
            
            docs.append(
                {
                    "chunk_id": len(docs),
                    "content": content,
                    "similarity": round(similarity, 4) if similarity is not None else None,
                    "metadata": metadata,
                }
            )
        return docs

    def _query_langgraph(self, user_id: int, question: str, thread_id: Optional[str] = None) -> Dict:
        """
        LangGraph RAG æŸ¥è¯¢ï¼ˆéæµå¼ï¼‰ï¼Œè¿”å›ä¸ç°æœ‰æ¥å£å…¼å®¹çš„ç»“æœå­—å…¸ã€‚
        
        é‡è¦è¯´æ˜ï¼š
        - retry_count æ˜¯å•æ¬¡è¯·æ±‚å†…çš„æ§åˆ¶å‚æ•°ï¼Œç”¨äºæ§åˆ¶æ–‡æ¡£æ£€ç´¢é‡è¯•æ¬¡æ•°ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
        - æ¯æ¬¡æ–°ç”¨æˆ·è¯·æ±‚æ—¶ï¼Œretry_count å¿…é¡»ä» 0 å¼€å§‹
        - checkpoint ä¼šè‡ªåŠ¨æ¢å¤å†å² messagesï¼Œä½† retry_count ä¼šè¢«æ–°å€¼è¦†ç›–
        
        Args:
            user_id: ç”¨æˆ· ID
            question: ç”¨æˆ·é—®é¢˜
            thread_id: å¯é€‰çš„ thread_idï¼Œç”¨äºå¤šè½®å¯¹è¯ï¼ˆcheckpointï¼‰
        """
        start_time = time.time()
        token_counter.reset()

        graph = self._build_langgraph_graph(user_id)
        
        # æ¯æ¬¡æ–°è¯·æ±‚æ—¶ï¼Œæ˜ç¡®é‡ç½®æ‰€æœ‰å•æ¬¡è¯·æ±‚ç›¸å…³çš„çŠ¶æ€å­—æ®µ
        initial_state = {
            "messages": [HumanMessage(content=question)],  # æ–°æ¶ˆæ¯ä¼šè¢« add_messages reducer è¿½åŠ åˆ°å†å²æ¶ˆæ¯ä¸­
            "retry_count": 0,  # æ¯æ¬¡æ–°è¯·æ±‚éƒ½é‡ç½®ä¸º 0ï¼ˆå•æ¬¡è¯·æ±‚å†…çš„é‡è¯•è®¡æ•°ï¼‰
            "current_query": question,  # å½“å‰æŸ¥è¯¢ï¼Œæ¯æ¬¡æ–°è¯·æ±‚éƒ½æ›´æ–°
            "no_relevant_found": False,  # æ¯æ¬¡æ–°è¯·æ±‚éƒ½é‡ç½®ä¸º False
        }

        # é…ç½® checkpoint
        graph_config = None
        if config.USE_CHECKPOINT:
            if not thread_id:
                # ä¸ºå•æ¬¡æŸ¥è¯¢ç”Ÿæˆä¸´æ—¶ thread_id
                thread_id = f"temp_{user_id}_{uuid.uuid4().hex[:8]}"
            graph_config = {"configurable": {"thread_id": thread_id}}
            logger.debug(f"[RAGService] ä½¿ç”¨ thread_id: {thread_id}ï¼Œretry_count å°†ä» 0 å¼€å§‹ï¼ˆè¦†ç›– checkpoint ä¸­çš„æ—§å€¼ï¼‰")
        elif thread_id:
            # å¦‚æœæä¾›äº† thread_id ä½†æœªå¯ç”¨ checkpointï¼Œä»ç„¶ä½¿ç”¨ï¼ˆå…¼å®¹æ€§ï¼‰
            graph_config = {"configurable": {"thread_id": thread_id}}

        final_state = graph.invoke(initial_state, config=graph_config)
        messages = final_state.get("messages", []) if isinstance(final_state, dict) else []

        answer = ""
        retrieved_docs: List[Dict] = []
        tool_text = ""
        for m in messages:
            if isinstance(m, ToolMessage):
                tool_text = str(getattr(m, "content", "") or "")
        if tool_text:
            retrieved_docs = self._parse_retrieve_output(tool_text)

        if messages:
            last = messages[-1]
            answer = str(getattr(last, "content", "") or "")

        elapsed_time = time.time() - start_time

        thinking_process = [
            {"step": 1, "action": "åˆ†æé—®é¢˜", "description": "è¿›å…¥ LangGraph RAG å·¥ä½œæµ", "details": f"é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦"},
            {"step": 2, "action": "æ£€ç´¢ä¸è¯„ä¼°", "description": "å·¥å…·æ£€ç´¢ + æ–‡æ¡£ç›¸å…³æ€§è¯„ä¼° + å¯èƒ½é‡å†™", "details": f"æ£€ç´¢åˆ° {len(retrieved_docs)} æ¡å†…å®¹ï¼ˆè§£æè‡ªå·¥å…·è¾“å‡ºï¼‰"},
            {"step": 3, "action": "ç”Ÿæˆç­”æ¡ˆ", "description": "åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”", "details": f"å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦"},
            {"step": 4, "action": "å®Œæˆ", "description": "å›ç­”ç”Ÿæˆå®Œæˆ", "details": f"è€—æ—¶: {elapsed_time:.2f} ç§’"},
        ]

        tokens_used = token_counter.total_stats.get("total_tokens", 0) or (len(question) // 4 + len(answer) // 4)

        return {
            "answer": answer,
            "retrieved_docs": retrieved_docs,
            "thinking_process": thinking_process,
            "elapsed_time": elapsed_time,
            "tokens_used": tokens_used,
            "fallback_mode": False,
        }

    def _query_langgraph_stream(self, user_id: int, question: str, thread_id: Optional[str] = None) -> Generator[Dict, None, None]:
        """
        LangGraph RAG æµå¼æŸ¥è¯¢ï¼Œä½¿ç”¨ graph.stream() å®ç°çœŸæ­£çš„æµå¼è¾“å‡ºã€‚
        
        é‡è¦è¯´æ˜ï¼š
        - retry_count æ˜¯å•æ¬¡è¯·æ±‚å†…çš„æ§åˆ¶å‚æ•°ï¼Œç”¨äºæ§åˆ¶æ–‡æ¡£æ£€ç´¢é‡è¯•æ¬¡æ•°ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
        - æ¯æ¬¡æ–°ç”¨æˆ·è¯·æ±‚æ—¶ï¼Œretry_count å¿…é¡»ä» 0 å¼€å§‹
        - checkpoint ä¼šè‡ªåŠ¨æ¢å¤å†å² messagesï¼Œä½† retry_count ä¼šè¢«æ–°å€¼è¦†ç›–
        - SummarizationMiddleware ä¼šåœ¨æ¶ˆæ¯è¶…è¿‡é˜ˆå€¼æ—¶è‡ªåŠ¨æ€»ç»“
        
        Args:
            user_id: ç”¨æˆ· ID
            question: ç”¨æˆ·é—®é¢˜
            thread_id: å¯é€‰çš„ thread_idï¼Œç”¨äºå¤šè½®å¯¹è¯ï¼ˆcheckpointï¼‰
        """
        start_time = time.time()
        token_counter.reset()

        graph = self._build_langgraph_graph(user_id)
        
        # æ¯æ¬¡æ–°è¯·æ±‚æ—¶ï¼Œæ˜ç¡®é‡ç½®æ‰€æœ‰å•æ¬¡è¯·æ±‚ç›¸å…³çš„çŠ¶æ€å­—æ®µ
        # è¿™äº›å­—æ®µä¸åº”è¯¥è·¨è¯·æ±‚ä¿æŒï¼Œæ¯æ¬¡æ–°è¯·æ±‚éƒ½ä»åˆå§‹å€¼å¼€å§‹
        initial_state = {
            "messages": [HumanMessage(content=question)],  # æ–°æ¶ˆæ¯ä¼šè¢« add_messages reducer è¿½åŠ åˆ°å†å²æ¶ˆæ¯ä¸­
            "retry_count": 0,  # æ¯æ¬¡æ–°è¯·æ±‚éƒ½é‡ç½®ä¸º 0ï¼ˆå•æ¬¡è¯·æ±‚å†…çš„é‡è¯•è®¡æ•°ï¼‰
            "current_query": question,  # å½“å‰æŸ¥è¯¢ï¼Œæ¯æ¬¡æ–°è¯·æ±‚éƒ½æ›´æ–°
            "no_relevant_found": False,  # æ¯æ¬¡æ–°è¯·æ±‚éƒ½é‡ç½®ä¸º False
        }

        # é…ç½® checkpoint
        graph_config = None
        if config.USE_CHECKPOINT:
            if not thread_id:
                # ä¸ºå•æ¬¡æŸ¥è¯¢ç”Ÿæˆä¸´æ—¶ thread_id
                thread_id = f"temp_{user_id}_{uuid.uuid4().hex[:8]}"
            
            # å…³é”®ï¼šä½¿ç”¨ config å‚æ•°æ˜ç¡®æŒ‡å®šè¦é‡ç½®çš„å­—æ®µ
            # LangGraph ä¼šå°† initial_state ä¸­çš„å€¼åº”ç”¨åˆ°çŠ¶æ€ä¸­
            # å¯¹äºæ²¡æœ‰ reducer çš„å­—æ®µï¼ˆå¦‚ retry_countï¼‰ï¼Œæ–°å€¼ä¼šè¦†ç›– checkpoint ä¸­çš„æ—§å€¼
            graph_config = {"configurable": {"thread_id": thread_id}}
            logger.debug(f"[RAGService] ä½¿ç”¨ thread_id: {thread_id}ï¼Œretry_count å°†ä» 0 å¼€å§‹ï¼ˆè¦†ç›– checkpoint ä¸­çš„æ—§å€¼ï¼‰")
        elif thread_id:
            # å¦‚æœæä¾›äº† thread_id ä½†æœªå¯ç”¨ checkpointï¼Œä»ç„¶ä½¿ç”¨ï¼ˆå…¼å®¹æ€§ï¼‰
            graph_config = {"configurable": {"thread_id": thread_id}}

        # ç”¨äºæ”¶é›†æœ€ç»ˆç»“æœ
        final_answer = ""
        retrieved_docs: List[Dict] = []
        tool_text = ""
        thinking_steps = []
        current_step = 1

        # æµå¼å¤„ç† graph è¾“å‡º
        logger.info(f"[RAGService] å¼€å§‹ LangGraph æµå¼æŸ¥è¯¢ï¼Œthread_id={thread_id}")
        logger.info(f"[RAGService] åˆå§‹çŠ¶æ€ - messages æ•°é‡: {len(initial_state['messages'])}")
        for msg in initial_state['messages']:
            logger.info(f"  åˆå§‹æ¶ˆæ¯: {type(msg).__name__} - {str(msg.content)[:100]}...")
        
        # å¦‚æœä½¿ç”¨ checkpointï¼Œå°è¯•è·å–å†å²æ¶ˆæ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if config.USE_CHECKPOINT and thread_id:
            try:
                # å°è¯•è·å–å†å²çŠ¶æ€ï¼ˆä»…ç”¨äºè°ƒè¯•ï¼Œä¸ä¿®æ”¹ï¼‰
                from langgraph.checkpoint.base import Checkpoint
                # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™… API å¯èƒ½ä¸åŒ
                logger.debug(f"[RAGService] ä½¿ç”¨ checkpointï¼Œthread_id={thread_id}")
            except Exception:
                pass
        
        for chunk in graph.stream(initial_state, config=graph_config):
            for node_name, node_update in chunk.items():
                logger.info(f"[RAGService] LangGraph èŠ‚ç‚¹æ›´æ–°: {node_name}")
                # å¤„ç†ä¸åŒèŠ‚ç‚¹çš„æ›´æ–°
                if node_name == "generate_query_or_respond":
                    thinking_steps.append({
                        "step": current_step,
                        "action": "åˆ†æé—®é¢˜",
                        "description": "ç”ŸæˆæŸ¥è¯¢æˆ–åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢",
                        "details": " åˆ¤æ–­æ˜¯å¦æ–‡æ¡£æ£€ç´¢ "
                    })
                    current_step += 1
                
                elif node_name == "retrieve":
                    # æå–å·¥å…·è¿”å›çš„æ–‡æ¡£
                    if "messages" in node_update:
                        for msg in node_update["messages"]:
                            if isinstance(msg, ToolMessage):
                                tool_text = str(getattr(msg, "content", "") or "")
                                if tool_text:
                                    retrieved_docs = self._parse_retrieve_output(tool_text)
                    
                    thinking_steps.append({
                        "step": current_step,
                        "action": "æ–‡æ¡£æ£€ç´¢",
                        "description": f"æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ®µè½",
                        "details": "å·¥å…·æ£€ç´¢å®Œæˆ"
                    })
                    current_step += 1
                
                elif node_name == "grade_documents":
                    thinking_steps.append({
                        "step": current_step,
                        "action": "è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§",
                        "description": "åˆ¤æ–­æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ç›¸å…³",
                        "details": "æ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°"
                    })
                    current_step += 1
                
                elif node_name == "rewrite_question":
                    thinking_steps.append({
                        "step": current_step,
                        "action": "é‡å†™é—®é¢˜",
                        "description": "ä¼˜åŒ–æŸ¥è¯¢ä»¥æé«˜æ£€ç´¢æ•ˆæœ",
                        "details": "é—®é¢˜é‡å†™"
                    })
                    current_step += 1
                
                elif node_name == "generate_answer":
                    # generate_answer èŠ‚ç‚¹å†…éƒ¨å·²ç»å®ç°äº†æµå¼è¾“å‡º
                    # ä½†è¿™é‡Œæˆ‘ä»¬åªèƒ½è·å–èŠ‚ç‚¹å®Œæˆåçš„æœ€ç»ˆæ¶ˆæ¯
                    if "messages" in node_update:
                        for msg in node_update["messages"]:
                            if hasattr(msg, "content"):
                                content = str(getattr(msg, "content", "") or "")
                                if content and not content.startswith("ç”¨æˆ·é—®é¢˜:"):  # æ’é™¤é‡è¯•æ—¶çš„æç¤º
                                    final_answer = content
                    
                    # ç”±äºèŠ‚ç‚¹å†…éƒ¨æµå¼è¾“å‡ºæ— æ³•åœ¨è¿™é‡Œæ•è·ï¼Œæˆ‘ä»¬é‡‡ç”¨æŠ˜ä¸­æ–¹æ¡ˆï¼š
                    # å¦‚æœç­”æ¡ˆå·²ç”Ÿæˆï¼Œç›´æ¥ yield å®Œæ•´ç­”æ¡ˆï¼ˆåç»­å¯ä»¥ä¼˜åŒ–ä¸ºçœŸæ­£çš„æµå¼ï¼‰
                    if final_answer:
                        thinking_steps.append({
                            "step": current_step,
                            "action": "ç”Ÿæˆç­”æ¡ˆ",
                            "description": "åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”",
                            "details": f"å›ç­”é•¿åº¦: {len(final_answer)} å­—ç¬¦"
                        })
                        current_step += 1

        elapsed_time = time.time() - start_time

        # æ„å»ºå®Œæ•´çš„ thinking_process
        if not thinking_steps:
            thinking_steps = [
                {"step": 1, "action": "åˆ†æé—®é¢˜", "description": "è¿›å…¥ LangGraph RAG å·¥ä½œæµ", "details": f"é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦"},
                {"step": 2, "action": "æ£€ç´¢ä¸è¯„ä¼°", "description": "å·¥å…·æ£€ç´¢ + æ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°", "details": f"æ£€ç´¢åˆ° {len(retrieved_docs)} æ¡å†…å®¹"},
                {"step": 3, "action": "ç”Ÿæˆç­”æ¡ˆ", "description": "åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”", "details": f"å›ç­”é•¿åº¦: {len(final_answer)} å­—ç¬¦"},
            ]
        
        thinking_steps.append({
            "step": len(thinking_steps) + 1,
            "action": "å®Œæˆ",
            "description": "å›ç­”ç”Ÿæˆå®Œæˆ",
            "details": f"è€—æ—¶: {elapsed_time:.2f} ç§’"
        })

        tokens_used = token_counter.total_stats.get("total_tokens", 0) or (len(question) // 4 + len(final_answer) // 4)

        # Yield æ€è€ƒè¿‡ç¨‹
        yield {"type": "thinking", "thinking_process": thinking_steps}

        # Yield ç­”æ¡ˆï¼ˆç”±äºèŠ‚ç‚¹å†…éƒ¨æµå¼æ— æ³•æ•è·ï¼Œè¿™é‡Œé‡‡ç”¨åˆ†ç‰‡æ–¹å¼ï¼‰
        # æ³¨æ„ï¼šè¿™æ˜¯æŠ˜ä¸­æ–¹æ¡ˆï¼ŒçœŸæ­£çš„æµå¼éœ€è¦ä¿®æ”¹èŠ‚ç‚¹å®ç°
        if final_answer:
            step = 50
            for i in range(0, len(final_answer), step):
                yield {"type": "chunk", "content": final_answer[i : i + step]}

        # æ‰“å°å®Œæ•´çš„ Token ç»Ÿè®¡
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š å®Œæ•´ Token ä½¿ç”¨ç»Ÿè®¡")
        logger.info("=" * 80)
        
        # æ€»ä½“ç»Ÿè®¡
        total_stats = token_counter.total_stats
        logger.info(f"\nã€æ€»ä½“ç»Ÿè®¡ã€‘")
        logger.info(f"  æ€»è°ƒç”¨æ¬¡æ•°: {total_stats.get('call_count', 0)}")
        logger.info(f"  æ€»è¾“å…¥ Token: {int(total_stats.get('input_tokens', 0)):,}")
        logger.info(f"  æ€»è¾“å‡º Token: {int(total_stats.get('output_tokens', 0)):,}")
        logger.info(f"  æ€» Token: {int(total_stats.get('total_tokens', 0)):,}")
        
        # æŒ‰èŠ‚ç‚¹ç»Ÿè®¡
        if token_counter.node_stats:
            logger.info(f"\nã€æŒ‰èŠ‚ç‚¹ç»Ÿè®¡ã€‘")
            for node_name, stats in token_counter.node_stats.items():
                logger.info(f"  {node_name}:")
                logger.info(f"    è°ƒç”¨æ¬¡æ•°: {stats.get('call_count', 0)}")
                logger.info(f"    è¾“å…¥ Token: {int(stats.get('input_tokens', 0)):,}")
                logger.info(f"    è¾“å‡º Token: {int(stats.get('output_tokens', 0)):,}")
                logger.info(f"    æ€» Token: {int(stats.get('total_tokens', 0)):,}")
        
        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        if token_counter.model_stats:
            logger.info(f"\nã€æŒ‰æ¨¡å‹ç»Ÿè®¡ã€‘")
            for model_name, stats in token_counter.model_stats.items():
                logger.info(f"  {model_name}:")
                logger.info(f"    è°ƒç”¨æ¬¡æ•°: {stats.get('call_count', 0)}")
                logger.info(f"    è¾“å…¥ Token: {int(stats.get('input_tokens', 0)):,}")
                logger.info(f"    è¾“å‡º Token: {int(stats.get('output_tokens', 0)):,}")
                logger.info(f"    æ€» Token: {int(stats.get('total_tokens', 0)):,}")
        
        logger.info("=" * 80 + "\n")
        
        # Yield å®Œæˆä¿¡æ¯
        yield {
            "type": "complete",
            "answer": final_answer,
            "retrieved_docs": retrieved_docs,
            "thinking_process": thinking_steps,
            "elapsed_time": elapsed_time,
            "tokens_used": tokens_used,
            "fallback_mode": False,
        }
    
    def query(self, user_id: int, question: str, k: int = None, thread_id: Optional[str] = None) -> Dict:
        """
        æ‰§è¡Œ RAG æŸ¥è¯¢
        
        Args:
            user_id: ç”¨æˆ· ID
            question: ç”¨æˆ·é—®é¢˜
            k: æ£€ç´¢æ•°é‡
            thread_id: å¯é€‰çš„ thread_idï¼Œç”¨äºå¤šè½®å¯¹è¯ï¼ˆcheckpointï¼‰
        
        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸ï¼š
            {
                'answer': str,
                'retrieved_docs': List[Dict],
                'thinking_process': List[Dict],
                'elapsed_time': float,
                'tokens_used': int
            }
        """
        # LangGraph è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        if config.USE_LANGGRAPH_RAG:
            return self._query_langgraph(user_id, question, thread_id=thread_id)

        start_time = time.time()
        
        # 1. å‘é‡æ£€ç´¢
        thinking_process = []
        thinking_process.append({
            'step': 1,
            'action': 'åˆ†æé—®é¢˜',
            'description': f'è¯†åˆ«é—®é¢˜ç±»å‹å¹¶æå–å…³é”®è¯',
            'details': f'é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦'
        })
        
        docs_with_scores = self.vector_service.search_with_score(user_id, question, k=k)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦é™çº§åˆ°ç›´æ¥å›ç­”
        should_fallback = False
        fallback_reason = ""
        
        if not docs_with_scores:
            # æƒ…å†µ Aï¼šæ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£
            should_fallback = True
            fallback_reason = "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
        elif config.RAG_FALLBACK_ENABLED:
            # æƒ…å†µ Bï¼šæ£€æŸ¥ç›¸ä¼¼åº¦é˜ˆå€¼
            max_similarity = max([max(0, 1 - score) for _, score in docs_with_scores])
            if max_similarity < config.RAG_SIMILARITY_THRESHOLD:
                should_fallback = True
                fallback_reason = f"ç›¸ä¼¼åº¦å¤ªä½ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.2f}ï¼‰"
        
        if should_fallback:
            # ä½¿ç”¨ç›´æ¥å›ç­”æ¨¡å¼
            thinking_process.append({
                'step': 2,
                'action': 'é™çº§åˆ°ç›´æ¥å›ç­”',
                'description': fallback_reason,
                'details': 'ä½¿ç”¨å¤§æ¨¡å‹ç›´æ¥å›ç­”ï¼Œä¸ä¾èµ–çŸ¥è¯†åº“'
            })
            
            thinking_process.append({
                'step': 3,
                'action': 'ç”Ÿæˆç­”æ¡ˆ',
                'description': 'ä½¿ç”¨å¤§æ¨¡å‹ç›´æ¥å›ç­”',
                'details': 'ä¸ä¾èµ–çŸ¥è¯†åº“å†…å®¹'
            })
            
            # ä½¿ç”¨ç›´æ¥å›ç­” Chain
            direct_chain = self.direct_prompt | self.llm | StrOutputParser()
            answer = direct_chain.invoke({"question": question})
            
            elapsed_time = time.time() - start_time
            
            thinking_process.append({
                'step': 4,
                'action': 'å®Œæˆ',
                'description': f'å›ç­”ç”Ÿæˆå®Œæˆ',
                'details': f'è€—æ—¶: {elapsed_time:.2f} ç§’'
            })
            
            # ä¼°ç®— Token æ¶ˆè€—ï¼ˆç›´æ¥å›ç­”æ²¡æœ‰ä¸Šä¸‹æ–‡ï¼‰
            estimated_tokens = len(question) // 4 + len(answer) // 4
            
            return {
                'answer': answer,
                'retrieved_docs': [],
                'thinking_process': thinking_process,
                'elapsed_time': elapsed_time,
                'tokens_used': estimated_tokens,
                'fallback_mode': True,
                'fallback_reason': fallback_reason
            }
        
        # 2. å¤„ç†æ£€ç´¢ç»“æœï¼ˆRAG æ¨¡å¼ï¼‰
        retrieved_docs = []
        context_parts = []
        
        for i, (doc, score) in enumerate(docs_with_scores):
            # è½¬æ¢è¯„åˆ†ä¸ºç›¸ä¼¼åº¦ï¼ˆChroma ä½¿ç”¨è·ç¦»ï¼Œè¶Šå°è¶Šç›¸ä¼¼ï¼‰
            similarity = max(0, 1 - score)  # ç®€å•è½¬æ¢
            
            retrieved_docs.append({
                'chunk_id': i,
                'content': doc.page_content,
                'similarity': round(similarity, 2),
                'metadata': doc.metadata
            })
            
            context_parts.append(f"[æ–‡æ¡£ç‰‡æ®µ {i+1}]\n{doc.page_content}")
        
        context = "\n\n".join(context_parts)
        
        avg_similarity = sum([d['similarity'] for d in retrieved_docs]) / len(retrieved_docs)
        thinking_process.append({
            'step': 2,
            'action': 'æ–‡æ¡£æ£€ç´¢',
            'description': f'æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ®µè½',
            'details': f'å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.2f}'
        })
        
        # 3. æ„é€  Prompt å¹¶è°ƒç”¨ LLM
        thinking_process.append({
            'step': 3,
            'action': 'ç”Ÿæˆç­”æ¡ˆ',
            'description': 'åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”',
            'details': f'ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦'
        })
        
        # ä½¿ç”¨ LangChain RAG Chain
        rag_chain = (
            {"context": lambda x: context, "question": RunnablePassthrough()}
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
        
        answer = rag_chain.invoke(question)
        
        elapsed_time = time.time() - start_time
        
        thinking_process.append({
            'step': 4,
            'action': 'å®Œæˆ',
            'description': f'å›ç­”ç”Ÿæˆå®Œæˆ',
            'details': f'è€—æ—¶: {elapsed_time:.2f} ç§’'
        })
        
        # TODO: è®¡ç®—å®é™… Token æ¶ˆè€—
        estimated_tokens = len(context) // 4 + len(question) // 4 + len(answer) // 4
        
        return {
            'answer': answer,
            'retrieved_docs': retrieved_docs,
            'thinking_process': thinking_process,
            'elapsed_time': elapsed_time,
            'tokens_used': estimated_tokens,
            'fallback_mode': False
        }
    
    def query_stream(self, user_id: int, question: str, k: int = None, thread_id: Optional[str] = None) -> Generator[Dict, None, None]:
        """
        æµå¼æ‰§è¡Œ RAG æŸ¥è¯¢
        
        Args:
            user_id: ç”¨æˆ· ID
            question: ç”¨æˆ·é—®é¢˜
            k: æ£€ç´¢æ•°é‡
        
        Yields:
            å­—å…¸ï¼ŒåŒ…å«ä¸åŒç±»å‹çš„ä¿¡æ¯ï¼š
            - type='thinking': æ€è€ƒè¿‡ç¨‹ä¿¡æ¯
              {
                  'type': 'thinking',
                  'thinking_process': List[Dict]
              }
            - type='chunk': ç­”æ¡ˆç‰‡æ®µ
              {
                  'type': 'chunk',
                  'content': str  # å¢é‡å†…å®¹
              }
            - type='complete': å®Œæˆä¿¡æ¯ï¼ˆæœ€åä¸€æ¡ï¼‰
              {
                  'type': 'complete',
                  'answer': str,  # å®Œæ•´ç­”æ¡ˆ
                  'retrieved_docs': List[Dict],
                  'thinking_process': List[Dict],
                  'elapsed_time': float,
                  'tokens_used': int
              }
        """
        # LangGraph è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼šå½“å‰ä¸ºäº†ä¿æŒæ¥å£å…¼å®¹ï¼Œé‡‡ç”¨â€œå…ˆæ±‚å®Œæ•´ç­”æ¡ˆï¼Œå†åˆ†ç‰‡è¾“å‡ºâ€çš„æ–¹å¼
        # LangGraph è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼šä½¿ç”¨çœŸæ­£çš„æµå¼è¾“å‡º
        if config.USE_LANGGRAPH_RAG:
            for response in self._query_langgraph_stream(user_id, question, thread_id=thread_id):
                yield response
            return

        start_time = time.time()
        
        # 1. å‘é‡æ£€ç´¢
        thinking_process = []
        thinking_process.append({
            'step': 1,
            'action': 'åˆ†æé—®é¢˜',
            'description': f'è¯†åˆ«é—®é¢˜ç±»å‹å¹¶æå–å…³é”®è¯',
            'details': f'é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦'
        })
        
        docs_with_scores = self.vector_service.search_with_score(user_id, question, k=k)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦é™çº§åˆ°ç›´æ¥å›ç­”
        should_fallback = False
        fallback_reason = ""
        
        if not docs_with_scores:
            # æƒ…å†µ Aï¼šæ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£
            should_fallback = True
            fallback_reason = "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
        elif config.RAG_FALLBACK_ENABLED:
            # æƒ…å†µ Bï¼šæ£€æŸ¥ç›¸ä¼¼åº¦é˜ˆå€¼
            max_similarity = max([max(0, 1 - score) for _, score in docs_with_scores])
            if max_similarity < config.RAG_SIMILARITY_THRESHOLD:
                should_fallback = True
                fallback_reason = f"ç›¸ä¼¼åº¦å¤ªä½ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.2f}ï¼‰"
        
        if should_fallback:
            # ä½¿ç”¨ç›´æ¥å›ç­”æ¨¡å¼ï¼ˆæµå¼ï¼‰
            thinking_process.append({
                'step': 2,
                'action': 'é™çº§åˆ°ç›´æ¥å›ç­”',
                'description': fallback_reason,
                'details': 'ä½¿ç”¨å¤§æ¨¡å‹ç›´æ¥å›ç­”ï¼Œä¸ä¾èµ–çŸ¥è¯†åº“'
            })
            
            thinking_process.append({
                'step': 3,
                'action': 'ç”Ÿæˆç­”æ¡ˆ',
                'description': 'ä½¿ç”¨å¤§æ¨¡å‹ç›´æ¥å›ç­”',
                'details': 'ä¸ä¾èµ–çŸ¥è¯†åº“å†…å®¹'
            })
            
            # å…ˆ yield æ€è€ƒè¿‡ç¨‹
            yield {
                'type': 'thinking',
                'thinking_process': thinking_process
            }
            
            # æµå¼ç”Ÿæˆç›´æ¥å›ç­”
            direct_chain = self.direct_prompt | self.llm | StrOutputParser()
            full_answer = ""
            for chunk in direct_chain.stream({"question": question}):
                full_answer += chunk
                yield {
                    'type': 'chunk',
                    'content': chunk
                }
            
            elapsed_time = time.time() - start_time
            
            thinking_process.append({
                'step': 4,
                'action': 'å®Œæˆ',
                'description': f'å›ç­”ç”Ÿæˆå®Œæˆ',
                'details': f'è€—æ—¶: {elapsed_time:.2f} ç§’'
            })
            
            # ä¼°ç®— Token æ¶ˆè€—ï¼ˆç›´æ¥å›ç­”æ²¡æœ‰ä¸Šä¸‹æ–‡ï¼‰
            estimated_tokens = len(question) // 4 + len(full_answer) // 4
            
            # æœ€å yield å®Œæ•´ç»“æœ
            yield {
                'type': 'complete',
                'answer': full_answer,
                'retrieved_docs': [],
                'thinking_process': thinking_process,
                'elapsed_time': elapsed_time,
                'tokens_used': estimated_tokens,
                'fallback_mode': True,
                'fallback_reason': fallback_reason
            }
            return
        
        # 2. å¤„ç†æ£€ç´¢ç»“æœï¼ˆRAG æ¨¡å¼ï¼‰
        retrieved_docs = []
        context_parts = []
        
        for i, (doc, score) in enumerate(docs_with_scores):
            # è½¬æ¢è¯„åˆ†ä¸ºç›¸ä¼¼åº¦ï¼ˆChroma ä½¿ç”¨è·ç¦»ï¼Œè¶Šå°è¶Šç›¸ä¼¼ï¼‰
            similarity = max(0, 1 - score)  # ç®€å•è½¬æ¢
            
            retrieved_docs.append({
                'chunk_id': i,
                'content': doc.page_content,
                'similarity': round(similarity, 2),
                'metadata': doc.metadata
            })
            
            context_parts.append(f"[æ–‡æ¡£ç‰‡æ®µ {i+1}]\n{doc.page_content}")
        
        context = "\n\n".join(context_parts)
        
        avg_similarity = sum([d['similarity'] for d in retrieved_docs]) / len(retrieved_docs)
        thinking_process.append({
            'step': 2,
            'action': 'æ–‡æ¡£æ£€ç´¢',
            'description': f'æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ®µè½',
            'details': f'å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.2f}'
        })
        
        # 3. æ„é€  Prompt å¹¶è°ƒç”¨ LLMï¼ˆæµå¼ï¼‰
        thinking_process.append({
            'step': 3,
            'action': 'ç”Ÿæˆç­”æ¡ˆ',
            'description': 'åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”',
            'details': f'ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦'
        })
        
        # å…ˆ yield æ€è€ƒè¿‡ç¨‹
        yield {
            'type': 'thinking',
            'thinking_process': thinking_process
        }
        
        # ä½¿ç”¨ LangChain RAG Chainï¼ˆæµå¼ï¼‰
        rag_chain = (
            {"context": lambda x: context, "question": RunnablePassthrough()}
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
        
        # æµå¼ç”Ÿæˆç­”æ¡ˆ
        full_answer = ""
        for chunk in rag_chain.stream(question):
            full_answer += chunk
            yield {
                'type': 'chunk',
                'content': chunk
            }
        
        elapsed_time = time.time() - start_time
        
        thinking_process.append({
            'step': 4,
            'action': 'å®Œæˆ',
            'description': f'å›ç­”ç”Ÿæˆå®Œæˆ',
            'details': f'è€—æ—¶: {elapsed_time:.2f} ç§’'
        })
        
        # TODO: è®¡ç®—å®é™… Token æ¶ˆè€—
        estimated_tokens = len(context) // 4 + len(question) // 4 + len(full_answer) // 4
        
        # æœ€å yield å®Œæ•´ç»“æœ
        yield {
            'type': 'complete',
            'answer': full_answer,
            'retrieved_docs': retrieved_docs,
            'thinking_process': thinking_process,
            'elapsed_time': elapsed_time,
            'tokens_used': estimated_tokens,
            'fallback_mode': False
        }
    
    def format_docs(self, docs) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨ä¸ºå­—ç¬¦ä¸²"""
        return "\n\n".join(doc.page_content for doc in docs)


# å…¨å±€ RAG æœåŠ¡å®ä¾‹
_rag_service: Optional[RAGService] = None


def get_rag_service() -> RAGService:
    """è·å–å…¨å±€ RAG æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service

